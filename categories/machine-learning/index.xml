<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Chenjing Blog</title>
    <link>https://mickeyding.github.io/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Chenjing Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Feb 2018 16:30:37 +0800</lastBuildDate>
    
	<atom:link href="https://mickeyding.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>机器学习（二）概率密度估计之非参数估计 </title>
      <link>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E4%B9%8B%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Mon, 19 Feb 2018 16:30:37 +0800</pubDate>
      
      <guid>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E4%B9%8B%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</guid>
      <description>二.非参数估计 2.1直方图估计 直方图估计概率密度函数基本思想： 将数据空间分成许多个子空间，每一个子空间大小为$△$，在每一个子空间内计算样本出现的个数$n_i$，样本总个数为N，则概率密度函数为：$$ p(x) = \frac{n_i}{N△}；$$
平滑因子:  图4 不同平滑因子（上：平滑因子过小，估计的概率密度函数有很多毛刺，噪声； 中：平滑因子适合的时候，估计的概率密度函数； 下：平滑因子过大，估计的概率密度函数误差增大)  缺点: 当数据空间的维数为D，每一维划分的子空间个数为M，则所需子空间个数为$M^D$， 该个数呈指数级增长。有两种方法可以解决这个问题，它们都是针对每一个输入样本$\widehat{x}$，而并非对整个训练样本事先划分好子空间。 这两种方法有相同的思路：在一个很小的区域R内，$$P(x) = \int _R p(x)dx \approx p(x)V \Rightarrow p(x) = \frac{P(x)}{V} = \frac{K}{NV}$$K可以理解成V内训练样本的个数。如果固定V，则产生了核方法。如果固定K，则产生了K近邻估计的方法。
2.2核方法 引入核函数: $$k(μ) \geqslant 0, V=\int k(μ) dμ = 1（积分也可不为1） \\ 则K(\widehat{x})=\sum_{i=1}^n k(x_i-\widehat{x}) \Rightarrow p(x) = \frac{1}{N} \sum_{i=1}^n k(x_i-\widehat{x}) $$ 上述表述没有直方图方法那么直观，举以下两个例子：
&amp;ndash; $k(μ)$如下定义：
$$k(μ) = \lbrace_{0, else}^{1 \ ( |u_i|&amp;lt;\frac{h}{2},i = 1,2 &amp;hellip; D)} \\V =\int k(μ) dμ = h^D $$ 如果μ是二维，则该积分表示以$\widehat{x}$为中心，长宽为h，高为1的长方体体积 。 $$K(\widehat{x})=\sum_{i=1}^n k(x_i-\widehat{x}) = \sum_{i=1}^n 1 (|x_i-\widehat{x}| &amp;lt; \frac{h}{2} ) \\ p(x) = \frac{K}{NV} = \frac{ \sum_{i=1}^n 1(|x_i-\widehat{x}| &amp;lt; \frac{h}{2})}{N*h^D}$$ $K(\widehat{x})$表示的是与$\widehat{x} $距离小于$\frac{h}{2}$的样本点的个数,如下图所示：</description>
    </item>
    
    <item>
      <title>机器学习（二）概率密度分布之预备知识和所用符号 </title>
      <link>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E4%B9%8B%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%92%8C%E6%89%80%E7%94%A8%E7%AC%A6%E5%8F%B7/</link>
      <pubDate>Mon, 19 Feb 2018 16:30:37 +0800</pubDate>
      
      <guid>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E4%B9%8B%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E5%92%8C%E6%89%80%E7%94%A8%E7%AC%A6%E5%8F%B7/</guid>
      <description>机器学习（二）所用到的符号如下:
   符号 含义     $L(\theta)$ θ的可能性，参考机器学习（一）条件概率   $E(\theta)$ $\theta$的对数可能性(Log-likelihood)   $\widehat{μ}$ μ的估计量   X 随机变量X   n 样本个数n   D 样本的维度   x 随机变量X的样本x   $x_i$ 样本x的i个样本点   $x_{trai}$ 训练样本   $x_{trai_i}$ 训练样本的第i个样本点   $\widehat{x}$ 输入样本    高斯分布预备知识总结（可以先跳过，必要时查找）
1.一维高斯分布公式 $$N(x|μ，σ^2) = \frac{1}{\sqrt{2π}*σ} exp{- \frac{(x-μ)^2}{2σ^2}}$$
2.多维高斯分布公式 $$ N(x|μ,Σ)=\frac{1}{(2π)^{\frac{D}{2}} |Σ|^{\frac{1}{2}}}exp{-\frac{1}{2}(x-μ)^T Σ^{-1}(x-μ) }$$
3.多维高斯分布数据的相关性 由于Σ是实对称矩阵，可以将Σ分解成特征向量和的形式： $$Σ = \sum_{i=1}^D λ_i μ_i μ_i^T :$$ 矩阵的逆： $$Σ^{-1} = \sum_{i=1}^D \frac{1}{λ_i} μ_i μ_i^T$$ 以2维高斯分布为例：</description>
    </item>
    
    <item>
      <title>机器学习（一）贝叶斯判别式</title>
      <link>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%A4%E5%88%AB%E5%BC%8F/</link>
      <pubDate>Tue, 13 Feb 2018 16:30:37 +0800</pubDate>
      
      <guid>https://mickeyding.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%A4%E5%88%AB%E5%BC%8F/</guid>
      <description>符号 含义     $C_k$ 第k类   p 概率密度   $P(C_k)$ 第k类的概率。本文中的概率密度和概率在公式推导时已严格区分   x 输入数据；可为训练样本（已知类别）或者待分类数据（未知类别）,为变量   $q$ 输入数据，有固定取值，非变量   m 类型总数    一.三个基本概率 1.1先验概率 根据经验得到的概率。比如$P(C_k)$：第k类的先验概率
1.2条件概率 $P（x|C_k）$: 在第k类中产生观察到的数据x的概率，表示了x是由第k类产生的可能性。
1.3后验概率 $P（C_k|x）$:输入数据x是第k类的概率。
1.4 三者关系 $$p(x,C_k) = p(x|C_k)*P(C_k) = P(C_k|x)p(x)$$
其中x是连续随机变量，注意$P(x) = 0$；表达式中采用的是概率密度函数。 $C$是离散随机变量，表达式中采用的是概率。
 1.具体参考Christopher M. Bishop，Pattern Recognition and Machine Learning，Springer, 2006 1.2.1节。
2.在第二节4.3生成模型和判别模型的比较中再来比较条件概率和后验概率。
 二.贝叶斯判别式最佳决策准测的推导 目标函数： 使错分输入数据x的概率最小。 $$图1 贝叶斯判别式最小化错分概率$$ 已知决策准测$x_0$, 当$x &amp;lt; x_0$，即$x \in R_1$，贝叶斯决策认为x属于$C_1$类，反之则为$C_2类。</description>
    </item>
    
  </channel>
</rss>